- each log entry has a unique id (offset) starting from a point (usually 0) and having the offset value be incremented by 1 after each new event
- so events can be ordered by the time in which it is created
- the oldest event will have the smallest offset value (0) and the newest event will have the largest current offset value
- logs can be replayed this way by sorting it by the offset value and process its data in the exact same order it originally happened
- we can write to the same file forever as it will be filled up with data, so after it passes a size, we will create a set of files for the log (segments)
- after segments get filled up, we can delete it/archive it/etc and spin up a new segment 
- this new segment will be called the active one since we're actively writing to it
- each segment contains a store file & and an index file. 
- store file contains the actual data of the log entry. we continuosly append to records to store file.
- index file acts as a pointer to the location of a log entry to store files to speed up reads. (usually the uniqueId that's an offset value)
- index file only contains 2 fields (the offset & the localization of record in store file)
ex. given an offset value.
  1. use the offset value to read the index file to get the location of the record in the store file. 
  2. use the location to get the record in the store file.

- due to index file's size being way smaller than the store file, (2 fields only and not the actual entry), we can memory-map these fields to reduce
read time since we don't need to read disk for every read request. During setup, we can memory map the index (offset -> record location), so we can 
easily get the location of the actual record fast. 

log parts 
- Record - data stored in our log
- Store - the files we store records in
- Index - the files we store index entries in
- Segment - abstraction that ties a store & an index together
- Log - the abstraction that ties all segments together